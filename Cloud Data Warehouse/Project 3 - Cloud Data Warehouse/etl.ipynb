{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Cloud Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Load DWH Params from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "LOG_DATA               = config.get('S3','LOG_DATA')\n",
    "LOG_JSONPATH           = config.get('S3','LOG_JSONPATH')\n",
    "SONG_DATA              = config.get('S3','SONG_DATA')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DWH_DB_USER            = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create clients for IAM, EC2, S3 and Redshift\n",
    "**Note**: We are creating these resources in the the **us-west-2** region. Choose the same region in the your AWS web console to the see these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Check out the log-data sources on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWHBucket =  s3.Bucket(\"udacity-dend\")\n",
    "log_data_files = [filename.key for filename in DWHBucket.objects.filter(Prefix='log-data')]\n",
    "log_data_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Check out the song_data sources on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWHBucket =  s3.Bucket(\"udacity-dend\")\n",
    "song_data_files = [filename.key for filename in DWHBucket.objects.filter(Prefix='song_data')]\n",
    "song_data_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toc-hr-collapsed": true
   },
   "source": [
    "# STEP 1: IAM ROLE\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 2:  Redshift Cluster\n",
    "\n",
    "- Create a [RedShift Cluster](https://console.aws.amazon.com/redshiftv2/home)\n",
    "- For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toc-hr-collapsed": true
   },
   "source": [
    "## 2.1 Describe the cluster to see its status\n",
    "- run this block several times until the cluster status becomes `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toc-hr-collapsed": true
   },
   "source": [
    "<h2> 2.2 Take note of the cluster <font color='red'> endpoint and role ARN </font> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>DO NOT RUN THIS unless the cluster status becomes \"Available\". Make ure you are checking your Amazon Redshift cluster in the **us-west-2** region. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Get endpoint & IAM role from cluster and update dwh.cfg accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "config.set('CLUSTER','host', DWH_ENDPOINT)\n",
    "config.set('IAM_ROLE','arn', DWH_ROLE_ARN)\n",
    "with open('dwh.cfg', 'w') as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## STEP 3: Open an incoming  TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 4: Make sure you can connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 5: Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS staging_events;\n",
    "DROP TABLE IF EXISTS staging_songs;\n",
    "DROP TABLE IF EXISTS songplays;\n",
    "DROP TABLE IF EXISTS users;\n",
    "DROP TABLE IF EXISTS song;\n",
    "DROP TABLE IF EXISTS artist;\n",
    "DROP TABLE IF EXISTS time;\n",
    "\n",
    "CREATE TABLE staging_events(\n",
    "    artist VARCHAR,\n",
    "    auth VARCHAR,\n",
    "    firstName VARCHAR,\n",
    "    gender CHAR(1),\n",
    "    itemInSession INTEGER,\n",
    "    lastName VARCHAR,\n",
    "    length DECIMAL,\n",
    "    level VARCHAR,\n",
    "    location VARCHAR,\n",
    "    method VARCHAR,\n",
    "    page VARCHAR,\n",
    "    registration BIGINT,\n",
    "    sessionID INTEGER,\n",
    "    song VARCHAR,\n",
    "    status INTEGER,\n",
    "    ts TIMESTAMP,\n",
    "    userAgent VARCHAR,\n",
    "    userId INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE staging_songs(\n",
    "    num_songs INTEGER,\n",
    "    artist_id VARCHAR,\n",
    "    artist_latitude DECIMAL,\n",
    "    artist_longitude DECIMAL,\n",
    "    artist_location VARCHAR,\n",
    "    artist_name VARCHAR,\n",
    "    song_id VARCHAR,\n",
    "    title VARCHAR,\n",
    "    duration DECIMAL,\n",
    "    year INTEGER \n",
    ");\n",
    "\n",
    "CREATE TABLE songplays(\n",
    "    songplay_id INTEGER IDENTITY(0,1) NULL sortkey,\n",
    "    start_time TIMESTAMP,\n",
    "    user_id INTEGER,\n",
    "    level VARCHAR,\n",
    "    song_id VARCHAR,\n",
    "    artist_id VARCHAR,\n",
    "    session_id INTEGER,\n",
    "    location VARCHAR,\n",
    "    user_agent VARCHAR)\n",
    "diststyle all;\n",
    "\n",
    "CREATE TABLE users(\n",
    "    user_id INTEGER  PRIMARY KEY distkey,\n",
    "    first_name VARCHAR,\n",
    "    last_name VARCHAR,\n",
    "    gender CHAR(1),\n",
    "    level VARCHAR \n",
    ");\n",
    "\n",
    "CREATE TABLE songs(\n",
    "    song_id VARCHAR  PRIMARY KEY,\n",
    "    title VARCHAR,\n",
    "    artist_id VARCHAR distkey,\n",
    "    year INTEGER,\n",
    "    duration DECIMAL \n",
    ");\n",
    "\n",
    "CREATE TABLE artists(\n",
    "    artist_id VARCHAR  PRIMARY KEY distkey,\n",
    "    name VARCHAR,\n",
    "    location VARCHAR,\n",
    "    latitude DECIMAL,\n",
    "    longitude DECIMAL \n",
    ");\n",
    "CREATE TABLE time(\n",
    "    start_time TIMESTAMP  PRIMARY KEY distkey,\n",
    "    hour INTEGER,\n",
    "    day INTEGER,\n",
    "    week INTEGER,\n",
    "    month INTEGER,\n",
    "    year INTEGER,\n",
    "    weekday INTEGER \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 6: Load Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from S3 into staging_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY staging_events \n",
    "    FROM {}\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE OFF\n",
    "    REGION 'us-west-2'\n",
    "    TIMEFORMAT AS 'epochmillisecs'\n",
    "    TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL\n",
    "    JSON {};\n",
    "\"\"\".format(LOG_DATA, DWH_ROLE_ARN, LOG_JSONPATH)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from staging_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from S3 into staging_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qry = \"\"\"\n",
    "    COPY staging_songs FROM {}\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE OFF region 'us-west-2'\n",
    "    FORMAT AS JSON 'auto' \n",
    "    TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL;\n",
    "\"\"\".format(SONG_DATA, DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from staging_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from staging_events & staging_songs into songplays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO songplays(start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "SELECT  TO_DATE(se.ts,'dd.mm.yyyy/hh:mi:ss') AS start_time,\n",
    "        se.userId                            AS user_id,\n",
    "        se.level                             AS level,\n",
    "        ss.song_id                           AS song_id,\n",
    "        ss.artist_id                         AS artist_id,\n",
    "        se.sessionID                         AS session_id,\n",
    "        se.location                          AS location,\n",
    "        se.userAgent                         AS user_agent\n",
    "FROM staging_events se\n",
    "JOIN staging_songs ss ON (se.song = ss.title) AND (se.artist = ss.artist_name)\n",
    "WHERE se.ts IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from staging_events into users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO users(user_id, first_name, last_name, gender, level)\n",
    "SELECT  se.userId    AS user_id,\n",
    "        se.firstName AS first_name,\n",
    "        se.lastName  AS last_name,\n",
    "        se.gender    AS gender,\n",
    "        se.level     AS level\n",
    "FROM staging_events se\n",
    "WHERE se.userId IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from staging_songs into songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO songs(song_id, title, artist_id, year, duration)\n",
    "SELECT  ss.song_id      AS song_id,\n",
    "        ss.title        AS title,\n",
    "        ss.artist_id    AS artist_id,\n",
    "        ss.year         AS year,\n",
    "        ss.duration     AS duration\n",
    "FROM staging_songs ss\n",
    "WHERE ss.song_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from staging_songs into artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO artists(artist_id, name, location, latitude, longitude)\n",
    "SELECT  ss.artist_id        AS artist_id,\n",
    "        ss.artist_name      AS name,\n",
    "        ss.artist_location  AS location,\n",
    "        ss.artist_latitude  AS latitude,\n",
    "        ss.artist_longitude AS longitude\n",
    "FROM staging_songs ss\n",
    "WHERE ss.artist_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from staging_events into time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO time(start_time, hour, day, month, week, year, weekday)\n",
    "SELECT  TO_DATE(se.ts,'dd.mm.yyyy/hh:mi:ss') AS start_time,\n",
    "        DATEPART(HOUR,start_time)            AS hour,\n",
    "        DATEPART(DAY,start_time)             AS day,\n",
    "        DATEPART(MONTH,start_time)           AS month,\n",
    "        DATEPART(WEEK,start_time)            AS week,\n",
    "        DATEPART(YEAR,start_time)            AS year,\n",
    "        DATEPART(WEEKDAY,start_time)         AS weekday\n",
    "FROM staging_events se\n",
    "WHERE se.ts IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# STEP 7: Clean up your resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><font color='red'>DO NOT RUN THIS UNLESS YOU ARE SURE <br/> \n",
    "    We will be using these resources in the next exercises</span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Verify that resources have been deleted properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- run this block several times until the cluster really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
